---
title: "Fraud_Analysis"
author: "Sarah Deussing, Ryan Steffe, Taylor Hill"
date: "2024-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction (5 points)(0.5 page): 

The problem we hope to understand is that of credit card fraud. Credit cards are swiped across the world and for purchases of different sizes. With our data, we hope to determine the factors that signify a fradulent transaction. To do so, we will be running several different models to "predict" fraud. 

Although our analysis includes several different predictive models, we understand that credit card fraud is not something typically predicted. With our predictive models, we want to understand the elements associated with fradulent transactions (i.e. have strong predictive power for fraud). With these insights, credit card companies, banks, and individual credit card holders can be more aware and able to identify when transactions from a card may be fradulent. For example, if age has a positive relationship with the odds of a fradulent transactions, awareness can be raised to older individuals. In selecting this dataset, we hope to increase our own awareness about the topic.

The inputs to our model are the location of the transaction (city, state), information about the time of the transcation (day, time of day), the "job" associated with the transcation, and the age of the individual cardholder.

# Explanatory Data Analysis 

## Dataset
```{r Packages}
library(tidyr)
library(tidyverse)
library(dplyr)
library(caret)
library(ggplot2)
library(readr)
library(xgboost)
library(stringr)
library(lubridate)
library(data.table)
library(plyr)
library(leaflet)
library(glmnet)
```

```{r Read in data}
fraud <- read.csv("fraud_data.csv")
summary(fraud)
```

Our 'fraud' dataset includes both categorical and numerical identifiers, in addition to a binary outcome variable 'is_fraud.' We have information about the date/time of the transaction, the amount, job, information about the merchant, and date of birth of the cardholder.

To perform our analyses, we have to create several different variables from the initial columns.
  1. Transaction Date
  2. Transaction Time
  3. Age
  4. Hour of Day
  5. Time of Day
  6. Day of Week

```{r Cleaning}
# Make date column
fraud[c('trans_date', 'trans_time')] <- str_split_fixed(fraud$trans_date_trans_time, " ", 2)
fraud$trans_date <- as.Date(fraud$trans_date, "%d-%m-%Y")

# Make age column
fraud$dob <- as.Date(fraud$dob, "%d-%m-%Y")
fraud$age <- floor(interval(fraud$dob, now()) / years(1))

# Make time column
fraud$time2 <- hm(fraud$trans_time)
fraud <- separate(fraud, trans_time, into = c("hours", "minutes"), sep = ":")
fraud$hours <- as.numeric(fraud$hours)
fraud$minutes <- as.numeric(fraud$minutes)
fraud$total_seconds <- (fraud$hours * 3600) + (fraud$minutes * 60) 
fraud$TimeOfDay <- ((fraud$hours*60)+fraud$minutes)/60

fraud$DayOfWeek <- wday(fraud$trans_date)
```

We also need to ensure that our response variable is truly binary. 
```{r Check response var}
unique(fraud$is_fraud)
```

We see some entries that contain information beyond '1' or '0'. For those entries, let's eliminate the extra information beyond the binary variable.

```{r Clean response var}
fraud$is_fraud <- substring(fraud$is_fraud, 1, 1)

fraud$is_fraud <- as.factor(fraud$is_fraud)
```

Let's understand our dataset by looking at the locations of our entries.
```{r Fraud Map}
pallette <- colorFactor(c("blue", "red"), domain = c(0, 1))

leaflet(fraud) %>%
  addTiles() %>%  
  addCircles(lng = ~long, lat = ~lat, 
             color = ~pallette(is_fraud), 
             radius = 5,  
             fillOpacity = 0.6, 
             popup = ~paste("Fraud Status:", is_fraud)) %>%
  addLegend("bottomright", 
            pal = pallette, 
            values = c(0, 1), 
            title = "Is Fraud", 
            labels = c("Not Fraud (0)", "Fraud (1)"))
```

It appears that our dataset is limited to solely the midwest/west coast. Therefore, any conclusions that we may find cannot be generalized beyond these locations.

## Exploratory Plots
The response variable is 'is_fraud'. 1 denotes fraud, 0 denotes not fraud.

Let's explore some of our variables and their relationship with our outcome variable. We will do so with:
  - Category
  - Amount
  - Job
  - Time
  - Age

```{r Exploratory Plots}
# 1. Category
ggplot(fraud, aes(x=category, group = is_fraud, fill = is_fraud)) + geom_bar() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = 'Count of Fraud in Transaction Categories', x = 'Category') 

# 2. Amount
ggplot(fraud, aes(x=amt, y=is_fraud)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = 'Fraud at Different Amounts', x = 'Amount') + 
  coord_cartesian(xlim = c(0, 1500))

# 3. Job
job_df <- fraud %>%
  filter(is_fraud == 1) %>%
  select(is_fraud, job) %>%
  group_by(job) %>%
  count() %>%
  mutate(freq = freq) %>%
  arrange(desc(freq)) %>%
  head(10)
ggplot(job_df, aes(x=reorder(job, -freq), y = freq)) + geom_col() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = 'Count of Fraud by Job', x = 'Job')

# 4. Time
# By Month
date_df <- fraud %>%
  filter(is_fraud == 1) %>%
  mutate(month = month(trans_date)) %>%
  select(is_fraud, month) %>%
  group_by(month) %>%
  count() %>%
  mutate(month_freq = freq)

ggplot(date_df, aes(x=month, y = month_freq)) + geom_col() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  labs(title = 'Count of Fraud Occurrences by Month',
       x = 'Month January-December')

# By Time of Day (Hour)
ggplot(fraud, aes(x=tod, color = is_fraud)) + geom_histogram(fill = "black") +
  theme(axis.text.x = element_text(angle = 90), panel.background = element_rect("black"), panel.grid.minor = element_blank(),
    panel.grid.major = element_blank()) +
  labs(title = 'Count of Fraud Occurrences by Time of Day',
       x = 'Time of Day')

# By Hour of Day
hour_df <- fraud %>%
  mutate(hour = total_seconds %/% 3600,
         is_fraud = as.numeric(is_fraud)) %>%
  group_by(hour) %>%
  summarize(count_fraud = sum(is_fraud))
ggplot(hour_df, aes(x=hour, y=count_fraud)) + geom_line() +
  labs(title = 'Count of Fraud Occurrences by Hour of Day',
       x = 'Hour') +
  theme_minimal()

# 5. Age
ggplot(fraud, aes(x=age, y=is_fraud)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = 'Fraud at Different Ages', x = 'Age')
```

1. Category: We see that the counts of fraud are split through several different categories.
2. Amount: We see that fradulent transactions typically are those with much larger amounts.
3. Job: Like with category, we see that the counts of fraud are split through several different job titles.
4. Time
  - Month: We don't see an easily identifiable trend of fradulent transactions across the months of the year.
  - Hour: We see a general increase in the number of fradulent transactions later in the day.
5. Age: We don't see a significant difference in the number of fradulent transactions for different age groups.

** explain why those vars were chosen for analysis
** need to do a plot between two attributes
** and maybe some summary stats of numerics vars


For the purpose of prediction, 'merchant' may have too many categories to use in our modeling. 
We have several choices of representing time of day (hour, period, exact time) and date (exact date, day of the week) in our modeling. 


# Learning Algorithm Training and Testing
We will run various types of models with our outcome variable: is_fraud.
The goal of these models is to understand factors that are representative of a fradulent transcation, so that they can be flagged in the future.

#### Balance the dataset / Split into training + testing sets
```{r Balance count}
count(fraud$is_fraud)
```

There are 12,601 cases of no-fraud and 1,845 cases of fraud in the dataset.
For our modeling, we will balance our dataset when we create the train and test sets. To do so, we will randomly sample 1,500 entries from each class to make the train set and 300 entries (of the remaining entries) from each class to make the test set.
```{r Make train}
fraud$id <- seq_len(nrow(fraud))
set.seed(1234567)

train1 <- fraud %>% 
  filter(is_fraud == 1) %>%
  sample_n(1500)

train0 <- fraud %>% 
  filter(is_fraud == 0) %>%
  sample_n(1500)

train_set <- bind_rows(train1, train0)

dim(train_set)
count(train_set$is_fraud)
```

```{r Make test}
train_ids <- train_set$id
remaining <- fraud %>% 
  filter(!(id %in% train_ids))

test1 <- remaining %>% 
  filter(is_fraud == 1) %>%
  sample_n(300)

test0 <- remaining %>% 
  filter(is_fraud == 0) %>%
  sample_n(300)

test_set <- bind_rows(test1, test0)

dim(test_set)
count(test_set$is_fraud)

# check identical rows
copies <- train_set %>%
  inner_join(test_set, by = names(train_set))
nrow(copies)
```

Now that we have randomly sampled the training and testing data to have equal number of is_fraud cases in each, we can move forward with our modeling. 


#### Logistic Regression Model
Because we have a binary response variable, we can run logistic regression. We are going to do this in 2 ways - with a regression model and then a lasso model.

Our categorical predictors are: merchant, category, city, state, job, DayOfWeek.
Our numerical predictors are:  amt, TimeOfDay, age.

To run a logistic regression model, we need to convert these categorical predictors to factors.
```{r Modify train/test sets for models}
train_set <- train_set %>%
  mutate(merchant = as.factor(merchant),
         category = as.factor(category),
         city = as.factor(city),
         state = as.factor(state),
         job = as.factor(job),
         DayOfWeek = as.factor(DayOfWeek))

test_set <- test_set %>%
  mutate(merchant = as.factor(merchant),
         category = as.factor(category),
         city = as.factor(city),
         state = as.factor(state),
         job = as.factor(job),
         DayOfWeek = as.factor(DayOfWeek))
```

```{r Log Model}
log_model <- glm(is_fraud ~ category + state + job + DayOfWeek + amt + TimeOfDay + age,
                 data = train_set, family = binomial)
predicted_probs <- predict(log_model, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
#table(predicted = predicted_classes, actual = fraud2$is_fraud)
confusionMatrix(as.factor(predicted_classes), as.factor(train_set$is_fraud))
```

Our model's balanced accuracy is 0.8633, meaning that our chosen predictors make a strong model. We see a sensitivity score of 0.9027, meaning that our model correctly identified 90% of no-fraud (0) cases. We see a similar specificity value of 0.8240. This means that our model correctly identified 65.58% of the fraud cases in the train set. 

Our model performed well on both cases of fraud within the train set.Let's test our model on the test set.
```{r Predict log test}
predicted_probs_test <- predict(log_model, type = "response", newdata = test_set)
predicted_classes_test <- ifelse(predicted_probs_test > 0.5, 1, 0)
#table(predicted = predicted_classes, actual = fraud2$is_fraud)
confusionMatrix(as.factor(predicted_classes_test), as.factor(test_set$is_fraud))
```

For our test set, we see a balanced accuracy of 0.8433, which is only 2% lower than that of our train set. Our sensitivity value is 0.8800, meaning that our model correctly identified 88% of the no-fraud cases. Our specificity value is only slightly lower; our model correctly identified 81% of the fraud cases in the test set.

Because our model performed generally the same on the test set as the train set, we are confident in our model's performance to predict fraud across other datasets/instances.

#### Lasso Logistic Regression Model

Unlike in our previous model, a lasso model will automatically select the best predictors for modeling. 
```{r Make x/y training sets}
train_set2 <- train_set %>%
  select(category, state, job, DayOfWeek, amt, TimeOfDay, age, is_fraud) %>%
  mutate(is_fraud = as.factor(is_fraud))
# Make 'dummy variables' from categorical predictors
dummy_data <- model.matrix(~ . - is_fraud, data = train_set2)
dummy_data <- dummy_data[, -1]

#x_train <- as.matrix(train_set2[, -which(names(train_set2) == "is_fraud")])
x_train <- as.matrix(dummy_data)
y_train <- as.numeric(train_set2$is_fraud) - 1

```

```{r Find the best lambda}
lambda_seq <- seq(from = 0.1, to = 10, by = 0.1)
cv_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_seq, family = "binomial")

best_lambda <- cv_model$lambda.1se
```

```{r Predict on train data}
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = (best_lambda), family = "binomial")
prediction <- predict(lasso_model, x_train, type="response")

prediction <- ifelse(prediction > 0.48, 1, 0)

xtab <- table(prediction, y_train)
result<-confusionMatrix(xtab)
result$overall[['Accuracy']]
```

We see an accuracy of 0.856 with our Lasso model. Let's use this model on our prediction set.
```{r Lasso Predict on test data}
test_set2 <- test_set %>%
  select(category, state, job, DayOfWeek, amt, TimeOfDay, age, is_fraud) %>%
  mutate(is_fraud = as.factor(is_fraud))
# Make 'dummy variables' from categorical predictors
dummy_data_test <- model.matrix(~ . - is_fraud, data = test_set2)
dummy_data_test <- dummy_data_test[, -1]

#x_train <- as.matrix(train_set2[, -which(names(train_set2) == "is_fraud")])
x_test <- as.matrix(dummy_data_test)
y_test <- as.numeric(test_set2$is_fraud) - 1
```

In order to run our lasso model, we have to ensure that the total dimensions/levels between the training and test sets are the same.
```{r Setdiff}
missing <- setdiff(colnames(dummy_data), colnames(dummy_data_test))
missing
```

We see a difference in 9 jobs in x_train that are not in x_test. In order to make the dimensions the same, we'll add these jobs into test_set with a value of 0.
```{r}
for (col in missing) {
  new_cols <- as.data.frame(matrix(0, nrow = nrow(dummy_data_test), ncol = length(missing)))
  colnames(new_cols) <- missing
  dummy_data_test <- cbind(dummy_data_test, new_cols) 
}

x_test_aligned <- dummy_data_test[, colnames(dummy_data), drop = FALSE]
dim(x_train)
dim(x_test_aligned)
```

Now that both sets have the same number of dimensions (196), we can predict on our new test set.
```{r Predict on test data}
prediction_test <- predict(lasso_model, newx = as.matrix(x_test_aligned), type="response")

prediction_test <- ifelse(prediction_test > 0.48, 1, 0)

xtab <- table(prediction_test, y_test)
result<-confusionMatrix(xtab)
result$overall[['Accuracy']]
```

We see an accuracy of 86.2% on our test set, which is equal to that of our train set. Because the model is optimized on the train set, having a similar accuracy for our test set predictions validates our accuracy. This means that our lasso model has learned the patterns of our data, rather than just overfitting to only the train data.

### XGBoost
With our logistic regression models above (regular glm and lasso glm), we assumed a linear relationship between our predictors and the log-odds of our outcome variable (is_fraud). An XGBoost model can handle non-linear relationships between our predictors and log-odds outcome. 

We will run an XGBoost model to analyze more complex, non-linear relationships in our data.

To find the optimal number of iterations, we will use a cross-validation xgboost model. In this model, we will run a total of 1,000 rounds, but stop after 50 rounds where the model does not improve its learning.
```{r Make xgboost train}
train_set3 <- train_set %>%
  mutate_if(is.character, as.factor) %>%
  select(category, state, job, DayOfWeek, amt, TimeOfDay, age, is_fraud)
vars <- model.matrix(~ . - is_fraud, data = train_set3)
train_set3$is_fraud <- as.numeric(as.character(train_set3$is_fraud))
# [, -which(names(train_set3) == "is_fraud")]
dtrain <- xgb.DMatrix(data = as.matrix(vars),
                      label = train_set3$is_fraud)
```

```{r Run xgboost cv}
bst_cv <- xgb.cv(data = dtrain, 
              nfold = 5,
               eta = 0.1, 
               nrounds = 1000, 
               early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20, 
               objective = "binary:logistic",
               eval_metric = "auc",
               eval_metric = "error")
```

The best iteration is 93 rounds.

```{r xgboost model}
bst_model <- xgboost(data = dtrain,
                     eta = 0.1,
               nrounds = 93, 
               verbose = 1, 
                print_every_n = 20, 
               objective = "binary:logistic", 
               eval_metric = "auc",
               eval_metric = "error")
```

```{r Predict on train xgboost}
prediction_bst <- predict(bst_model, as.matrix(vars), type="response")
prediction_bst <- ifelse(prediction_bst > 0.5, 1, 0)

xtab <- table(prediction_bst, train_set3$is_fraud)
result<-confusionMatrix(xtab)
result$overall[['Accuracy']]
```

We see an accuracy of 99.3%, which implies that our model is overfitting to our train set. Let's test this out on the test set.
```{r XGBoost predict on test}
test_set3 <- test_set %>%
  mutate_if(is.character, as.factor) %>%
  select(category, state, job, DayOfWeek, amt, TimeOfDay, age, is_fraud)
vars_test <- model.matrix(~ . - is_fraud, data = test_set3)
test_set3$is_fraud <- as.numeric(as.character(test_set3$is_fraud))
```

We see the same difference in 9 jobs in vars that are not in vars_test (as we did in the Lasso model). In order to make the dimensions the same, we'll add these jobs into test_set with a value of 0, in the same way we did in the lasso model.
```{r Impute missing for xgboost}
missing <- setdiff(colnames(vars), colnames(vars_test))
for (col in missing) {
  new_cols <- as.data.frame(matrix(0, nrow = nrow(vars_test), ncol = length(missing)))
  colnames(new_cols) <- missing
  vars_test <- cbind(vars_test, new_cols) 
}

vars_test_aligned <- vars_test[, colnames(vars), drop = FALSE]
```

```{r Predict on test set xgboost}
prediction_bst <- predict(bst_model, newdata = as.matrix(vars_test_aligned))
prediction_bst <- ifelse(prediction_bst > 0.6, 1, 0)

xtab <- table(prediction_bst, test_set3$is_fraud)
result<-confusionMatrix(xtab)
result$overall[['Accuracy']]
```

We see an accuracy of 0.98 for our XGBoost model on the test set. 


## Time series

# Conclusion